begin_date,
"&end_date=",
end_date,
"&facet_filter=true&api-key=",
Sys.getenv("NYTIMES_KEY"))
initialQuery <- fromJSON(baseurl)
articleSearch <- function(search_terms, search_from_date, search_to_date) {
baseurl <- paste0(
"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",
search_terms,
"&begin_date=",
search_from_date,
"&end_date=",
search_to_date,
"&facet_filter=true&api-key=",
Sys.getenv("NYTIMES_KEY"))
fromJSON(baseurl)
}
query <- articleSearch("dragonfruit","20181001","20181031")
Glimpse(query)
glimpse(query)
query$response
query <- articleSearch("dragonfruit","20171001","20181031")
query$response
#Note: I thought I could put this all inside the articleSearch funciton, but kept getting kicked out of the API with various error messages.
getAllPages <- function(initialQuery){
hits <- initialQuery$response$meta$hits
numPages <- round((hits/10)-1)
pages <- list()
for(i in 0:numPages){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(2)
}
allNYTSearch <- rbind_pages(pages)
}
getAllPages(query)
articleSearch <- function(search_terms, search_from_date, search_to_date) {
baseurl <- paste0(
"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",
search_terms,
"&begin_date=",
search_from_date,
"&end_date=",
search_to_date,
"&facet_filter=true&api-key=",
Sys.getenv("NYTIMES_KEY"))
initialQuery <- fromJSON(baseurl)
hits <- initialQuery$response$meta$hits
numPages <- round((hits/10)-1)
pages <- list()
for(i in 0:numPages){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(2)
}
allNYTSearch <- rbind_pages(pages)
}
query <- articleSearch("dragonfruit","20171001","20181031")
articleSearch <- function(search_terms, search_from_date, search_to_date) {
baseurl <- paste0(
"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",
search_terms,
"&begin_date=",
search_from_date,
"&end_date=",
search_to_date,
"&facet_filter=true&api-key=",
Sys.getenv("NYTIMES_KEY"))
}
baseurl <- articleSearch("dragons","20171001","20181031")
initialQuery <- fromJSON(baseurl)
hits <- initialQuery$response$meta$hits
numPages <- round((hits/10)-1)
pages <- list()
for(i in 0:numPages){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(2)
}
allNYTSearch <- rbind_pages(pages)
#Note: I thought I could put this all inside the articleSearch function, but kept getting kicked out of the API with various error messages, mostly 429.
#Not sure why this was happening when the Sys.sleep seemed to be working!
getAllPages <- function(baseurl){
initialQuery <- fromJSON(baseurl)
hits <- initialQuery$response$meta$hits
numPages <- round((hits/10)-1)
pages <- list()
for(i in 0:numPages){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
print(Sys.time())
pages[[i+1]] <- nytSearch
Sys.sleep(2)
}
allNYTSearch <- rbind_pages(pages)
}
remove(allNYTSearch)
remove(baseurl)
#Note: I thought I could put this all inside the articleSearch function, but
#I kept getting kicked out of the API with various error messages, mostly 429.
#Not sure why this was happening when the Sys.sleep seemed to be working!
getAllPages <- function(search_uri){
initialQuery <- fromJSON(search_uri)
hits <- initialQuery$response$meta$hits
numPages <- round((hits/10)-1)
pages <- list()
for(i in 0:numPages){
nytSearch <- fromJSON(paste0(search_uri, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
print(Sys.time())
pages[[i+1]] <- nytSearch
Sys.sleep(2)
}
allNYTSearch <- rbind_pages(pages)
}
baseurl <- articleSearch("dragonfruit","20171001","20181031")
getAllPages(baseurl)
#Note: I thought I could put this all inside the articleSearch function, but
#I kept getting kicked out of the API with various error messages, mostly 429.
#Not sure why this was happening when the Sys.sleep seemed to be working!
getAllPages <- function(search_uri){
initialQuery <- fromJSON(search_uri)
hits <- initialQuery$response$meta$hits
numPages <- round((hits/10)-1)
pages <- list()
for(i in 0:numPages){
nytSearch <- fromJSON(paste0(search_uri, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
print(Sys.time())
pages[[i+1]] <- nytSearch
Sys.sleep(2)
}
allNYTSearch <- rbind_pages(pages)
}
baseurl <- articleSearch("dragonfruit","20171001","20181031")
allNYTSearch <- getAllPages(baseurl)
articleSearch <- function(search_terms, search_from_date, search_to_date) {
baseurl <- paste0(
"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",
search_terms,
"&begin_date=",
search_from_date,
"&end_date=",
search_to_date,
"&facet_filter=true&api-key=",
Sys.getenv("NYTIMES_KEY"))
}
baseurl <- articleSearch("dragons","20171001","20181031")
initialQuery <- fromJSON(baseurl)
hits <- initialQuery$response$meta$hits
numPages <- round((hits/10)-1)
pages <- list()
for(i in 0:numPages){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
message("Retrieving page ", i)
pages[[i+1]] <- nytSearch
Sys.sleep(2)
}
baseurl <- paste0(
"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",
q_term,
"&begin_date=",
begin_date,
"&end_date=",
end_date,
"&facet_filter=true&api-key=",
Sys.getenv("NYTIMES_KEY"))
initialQuery <- fromJSON(baseurl)
View(initialQuery)
articleSearch <- function(search_terms, search_from_date, search_to_date) {
baseurl <- paste0(
"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",
search_terms,
"&begin_date=",
search_from_date,
"&end_date=",
search_to_date)
initialQuery <- fromJSON(paste0(
baseurl,
"&facet_filter=true&api-key=",
Sys.getenv("NYTIMES_KEY")))
hits <- initialQuery$response$meta$hits
numPages <- round((hits/10)-1)
pages <- list()
Sys.sleep(1)
for(i in 0:numPages){
nytSearch <- fromJSON(
paste0(
baseurl,
"&page=",
i,
"&api-key=",
Sys.getenv("NYTIMES_KEY")),
flatten = TRUE) %>%
data.frame()
message("Retrieving page ", i)
message(Sys.time())
pages[[i+1]] <- nytSearch
Sys.sleep(2)
}
allNYTSearch <- rbind_pages(pages)
}
allNYTSearch <- articleSearch("dragons","20180101","20181031")
# Visualize coverage by source
allNYTSearch %>%
group_by(response.docs.source) %>%
summarize(count=n()) %>%
ggplot() +
geom_bar(aes(y=count,
x=response.docs.source,
fill=response.docs.source),
stat = "identity") +
coord_flip()
cronuts2018 <- articleSearch("cronuts","20180101","20181027")
cronuts2018$response.docs.snippet[1:3]
cakepops2018 <- articleSearch("cake+pops","20180101","20181027")
cakepops2018$response.docs.snippet[1:3]
cakepops2018$response.meta.hits
cronuts2018$response.meta.hits[1]
# Visualize coverage by type
cakepops2018 %>%
group_by(response.docs.type_of_material) %>%
summarize(count=n()) %>%
ggplot() +
geom_bar(aes(y=count,
x=response.docs.type_of_material,
fill=response.docs.type_of_material),
stat = "identity") +
coord_flip()
#Create a function
sumBarChart <- function(dataframe, summary_stat){
dataframe %>%
group_by(summary_stat) %>%
summarize(count=n()) %>%
ggplot() +
geom_bar(aes(y=count,
x=summary_stat,
fill=summary_stat),
stat = "identity") +
coord_flip()
}
sumBarChart(cronuts2018, response.doc.type_of_material)
#Create a function
sumBarChart <- function(dataframe, summary_stat){
dataframe %>%
group_by(summary_stat) %>%
summarize(count=n()) %>%
ggplot() +
geom_bar(aes(y=count,
x=summary_stat,
fill=summary_stat),
stat = "identity") +
coord_flip()
}
sumBarChart(cronuts2018, response.doc.type_of_material)
sumBarChart(cronuts2018, response.docs.type_of_material)
sumBarChart(cronuts2018, response.docs.source)
#Create a function
sumBarChart <- function(dataframe, summary_stat) {
dataframe %>%
group_by(summary_stat) %>%
summarize(count=n()) %>%
ggplot() +
geom_bar(aes(y=count,
x=summary_stat,
fill=summary_stat),
stat = "identity") +
coord_flip()
}
sumBarChart(cronuts2018, response.docs.source)
#Create a function
sumBarChart <- function(dataframe) {
dataframe %>%
group_by(response.docs.type_of_material) %>%
summarize(count=n()) %>%
ggplot() +
geom_bar(aes(y=count,
x=response.docs.type_of_material,
fill=response.docs.type_of_material),
stat = "identity") +
coord_flip()
}
sumBarChart(cronuts2018)
#Create a function
sumBarChart <- function(dataframe) {
dataframe %>%
group_by(response.docs.type_of_material) %>%
summarize(count=n()) %>%
ggplot() +
geom_bar(aes(y=count,
x=response.docs.type_of_material,
fill=response.docs.type_of_material),
stat = "identity") +
coord_flip() +
labs(x="Number of Articles", y="Type of Coverage")
}
sumBarChart(cronuts2018)
#Create a function
sumBarChart <- function(dataframe) {
dataframe %>%
group_by(response.docs.type_of_material) %>%
summarize(count=n()) %>%
ggplot() +
geom_bar(aes(y=count,
x=response.docs.type_of_material,
fill=response.docs.type_of_material),
stat = "identity") +
coord_flip() +
labs(x="Number of Articles", y="Type of Coverage")+
theme(legend.position="none")
}
sumBarChart(cronuts2018)+ggtitle("NYTimes Cronut Coverage, 2018")
sumBarChart(cakepops2018)+ggtitle("NY Times Cronut Coverage, 2018")
#Create a function
sumBarChart <- function(dataframe) {
dataframe %>%
group_by(response.docs.type_of_material) %>%
summarize(count=n()) %>%
ggplot() +
geom_bar(aes(y=count,
x=response.docs.type_of_material,
fill=response.docs.type_of_material),
stat = "identity") +
coord_flip() +
labs(x="Number of Articles", y="Type of Coverage")+
theme(legend.position="none")
}
sumBarChart(cronuts2018)+ggtitle("NY Times Cronut Coverage, 2018")
sumBarChart(cakepops2018)+ggtitle("NY Times Cake Pop Coverage, 2018")
install.packages("tm")
library(tm)
setwd(/Documents/GitHub/DATA607_Proj4/Project-4)
setwd(~/Documents/GitHub/DATA607_Proj4/Project-4)
setwd(Documents/GitHub/DATA607_Proj4/Project-4)
setwd(/Documents/GitHub/DATA607_Proj4/Project-4)
setwd("/Documents/GitHub/DATA607_Proj4/Project-4"")
setwd("/Documents/GitHub/DATA607_Proj4/Project-4")
setwd("~/Documents/GitHub/DATA607_Proj4/Project-4")
setwd("~/Documents/GitHub/DATA607_Proj4/")
Spam  <-Corpus(DirSource("/data/Spam"), readerControl = list(language="lat"))
spam  <-Corpus(DirSource("/data/spam"), readerControl = list(language="lat"))
setwd("~/Documents/GitHub/DATA607_Proj4/")
spam  <-Corpus(DirSource("/data/spam"), readerControl = list(language="lat"))
spam  <-Corpus(DirSource("~Documents/GitHub/DATA607_Proj4/data/spam"), readerControl = list(language="lat"))
spam_source <- DirSource("/data/spam") #input path for documents
spam_source <- DirSource("~/Documents/GitHub/DATA607_Proj4/data/spam") #input path for documents
spam <- Corpus(spam_source, readerControl=list(reader=readPlain)
Summary(spam)
spam_source <- DirSource("~/Documents/GitHub/DATA607_Proj4/data/spam") #input path for documents
spam <- Corpus(spam_source, readerControl=list(reader=readPlain))
Summary(spam)
spam_source <- DirSource("~/Documents/GitHub/DATA607_Proj4/data/spam") #input path for documents
spam <- Corpus(spam_source, readerControl=list(reader=readPlain))
ham_source <- DirSource("~/Documents/GitHub/DATA607_Proj4/data/easy_ham") #input path for documents
ham <- Corpus(ham_source, readerControl=list(reader=readPlain))
ham[[1]]
ham[[2]]
summary(ham)
head(ham)
str(ham)
spam_stem <- stemDocument(spam)
> dtm <- DocumentTermMatrix(spam)
dtm <- DocumentTermMatrix(spam)
spam <- tm_map(spam, stripWhitespace)
spam <- tm_map(spam, content_transformer(toLower))
spam <- tm_map(spam, content_transformer(tolower))
spam <- tm_map(spam, removeWords, stopwords("english"))
#get file names
ham_names<-dir("~/Documents/GitHub/DATA607_Proj4/data/easy_ham")
spam_names<-dir("~/Documents/GitHub/DATA607_Proj4/data/spam")
head(spam_names)
help("dir")
install.packages("RTextTools")
inspect(spam_source)
head(spam_source)
setwd("~/Documents/GitHub/DATA607_Proj4/")
spam_source <- DirSource("data/spam") #input path for documents
head(spam_source)
head(ham_names)
help("c")
#Create an empty vector
ham_message <- c()
#Use a for loop to paste each message into the list
for(i in 1:length(spam_source)) {
connection <- file(file, open="rt", encoding="latin1")
text <- readLines(connection)
msg <- text[seq(which(text=="")[1]+1,length(text),1)]
close(connection)
vector<-c(i,paste(msg, collapse=" "))
ham_message<-rbind(ham_message,vector)
}
#Create an empty vector
ham_message <- c()
#Use a for loop to paste each message into the list
for(i in 1:length(spam_source)) {
connection <- file(spam_source, open="rt", encoding="latin1")
text <- readLines(connection)
msg <- text[seq(which(text=="")[1]+1,length(text),1)]
close(connection)
vector<-c(i,paste(msg, collapse=" "))
ham_message<-rbind(ham_message,vector)
}
help("file")
spam_source <- DirSource("~/Documents/GitHub/DATA607_Proj4/data/spam") #input path for spam documents
#Create an empty vector
ham_message <- c()
#Use a for loop to paste each message into the list
for(i in 1:length(spam_source)) {
connection <- file(spam_source, open="rt", encoding="latin1")
text <- readLines(connection)
msg <- text[seq(which(text=="")[1]+1,length(text),1)]
close(connection)
vector<-c(i,paste(msg, collapse=" "))
ham_message<-rbind(ham_message,vector)
}
#Create an empty vector
ham_message <- c()
#Use a for loop to paste each message into the list
for(i in 1:length(spam_source)) {
connection <- file(spam_source[i], open="rt", encoding="latin1")
text <- readLines(connection)
msg <- text[seq(which(text=="")[1]+1,length(text),1)]
close(connection)
vector<-c(i,paste(msg, collapse=" "))
ham_message<-rbind(ham_message,vector)
}
help("text")
msg <- readlines(spam_source[1])
msg <- readLines(spam_source[1])
msg
msg <- readLines(spam_source[[1]])
mes
msg
msg <- readLines(file(spam_source[[1]]))
msg
spam_source[1]
tmp <- text(spam_source[1])
tmp <- readLines(file(spam_source[1]))
remove(list = ls())
spam_source <- DirSource("data/spam") #input path for spam documents
ham_source <- DirSource("data/easy_ham") #input path for ham documents
spam_corpus <- Corpus(VectorSource(spam_source))
spam_corpus
help("readLines")
tmp <- readLines(file(spam_source[1]), encoding = "Latin1")
tmp
tmp <- readLines(file(spam_source[[1]]), encoding = "Latin1")
spam_source <- dir("data/spam") #input path for spam documents
spam_source <- dir("~/Documents/GitHub/DATA607_Proj4/data/spam") #input path for spam documents
ham_dir <- "~/Documents/GitHub/DATA607_Proj4/data/easy_ham"
spam_dir <- "~/Documents/GitHub/DATA607_Proj4/data/spam"
spam_source[1]
connection <- paste0(spam_dir, spam_source[[1]])
connection <- paste0(spam_dir,"/", spam_source[[1]])
tmp <- readLines(connection)
tmp
tmp <- read.table(connection)
head(tmp)
connection <- paste0(spam_dir,"/", spam_source[[1]], ".txt")
tmp <- readLines(connection)
connection <- paste0(spam_dir,"/", spam_source[[2]])
tmp <- readLines(connection)
tmp
library(stringr)
tmp <- readLines(connection)
tmp <- str_c(tmp, collapse = "")
tmp
spam_source[[2]]
connection <- paste0(spam_dir,"/", spam_source[[3]])
tmp <- readLines(connection)
tmp
tmp <- readLines(str_c(spam_dir, "/", spam_source[[2]]))
tmp <- str_c(tmp, collapse = "")
msg <- str_detect(tmp, "Subject")
msg
msg <- str_locate(tmp, "Subject")
msg
str_subset()
msg_start[1]
tmp <- readLines(str_c(spam_dir, "/", spam_source[[2]]))
tmp <- str_c(tmp, collapse = "")
msg_start <- str_locate(tmp, "Subject")
msg_start[1]
msg <- str_extract(tmp, msg_start[1])
msg <- substr(tmp, msg_start[1])
tmp <- readLines(str_c(spam_dir, "/", spam_source[[2]]))
tmp <- str_c(tmp, collapse = "")
msg_start <- str_locate(tmp, "Subject")
msg <- str_sub(tmp, msg_start[1], -1)
msg
n <- 1
for (i in 2:length(list.files(spam_dir))){
tmp <- readLines(str_c(spam_dir, "/", spam_source[[i]]))
msg_start <- str_locate(tmp, "Subject")
msg <- str_sub(tmp, msg_start[1], -1)
if (length(msg) != 0) {
n <- n+1
tmp_corpus <- Corpus(VectorSource(msg))
spam_corpus <- c(spam_corpus, tmp_corpus)
}
}
head(spam_corpus)
remove(list = ls())
setwd("~/Documents/GitHub/DATA607_Proj4/")
spam_dir <- "~/Documents/GitHub/DATA607_Proj4/data/spam"
spam_source <- dir("~/Documents/GitHub/DATA607_Proj4/data/spam") #input path for spam documents
ham_dir <- "~/Documents/GitHub/DATA607_Proj4/data/easy_ham"
ham_source <- DirSource("data/easy_ham") #input path for ham documents
