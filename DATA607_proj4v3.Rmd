---
title: "Classifying Emails as Ham or Spam"
subtitle: "DATA607 Project 4"
author: "Alice Friedman"
date: "November 3, 2018"
output: 
  html_document:
    theme: yeti
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
remove(list=ls())
```
  
#Overview
The assigned task is to develop a model or set of models, train them to classify a corpus of emails as either "ham" or "spam," and then evaluate the results.

In order to to this, we will:

1. Create a document set from the Apache Spam Assasin library

2. Use the ```tm``` package to create a "corpus", perform some light data cleaning, and create a Document Term Matrix

3. Use ```RTextTools``` to create a container, train a model, and evaluate the results.

#Part 1: Creating a document set
  
We will first use some basic web scraping to download and untar the source files from the <a href="https://spamassassin.apache.org/old/publiccorpus/" target="_blank">Apache Spam Assassin</a> website.
```{r, results='hide', message=FALSE, warning=FALSE}
library(XML)
library(rvest)
library(RCurl)
library(stringr)
```

##Step 1: Scrape web to create list of files to download
```{r}
url <- "https://spamassassin.apache.org/old/publiccorpus/"

https_doc <- getURLContent(url) #necessary to use getURL from RCurl because of https

links <- getHTMLLinks(https_doc) #scrape url for links

filenames <- links[str_detect(links, ".tar.bz2")] #store links as file names, limiting results to those of interest

filenames_list <- as.list(filenames) #convert to list
```

##Step 2: Create function to download multiple CSVs from URL list
This program will not download files if they already exist in the specified folder

```{r}
download <- function(filename, baseurl, folder) {
  dir.create(folder, showWarnings = FALSE)
  fileurl <- str_c(baseurl, filename)
  if(!file.exists(str_c(folder, "/", filename))) {
    download.file(fileurl, 
                  destfile = str_c(folder, "/", filename))
    Sys.sleep(1)
  }
}
```


##Step 3: Run function using l_ply over scraped urls to download CSVs
This program will not download files if they already exist in the specified folder

```{r}
library(plyr)
setwd('~/Documents/GitHub/DATA607_Proj4/data') #set working drive

l_ply(filenames_list, download, 
      baseurl = url,
      folder = "downloads")
```

```{r}
setwd('~/Documents/GitHub/DATA607_Proj4/data')
folders <- c("easy_ham/", "spam_2/")

#Create a list of files in the folders
email_list <- list.files(folders, full.names = TRUE)
#Create a random sample of 1000
set.seed(123)
email_list <- sample(email_list, 1000)

library(stringr)
library(tm)


tmp <- readLines(email_list[[1]])
tmp <- str_c(tmp, collapse = " ")
tmp <- str_replace_all(tmp, pattern = "[[:punct:]]", replacement = " ")
tmp <- tolower(tmp)
email <- tmp
spam <- if(str_detect(email_list[[1]], "spam")) {"spam"} else {"ham"}
spam_labels <- spam
corpus <- VCorpus(VectorSource(email))
```


##Create for loop to add remaining files to the corpus
```{r}
for (i in 2:length(email_list)){
  tmp <- readLines(email_list[[i]])
  tmp <- str_c(tmp, collapse = " ")
  tmp <- str_replace_all(tmp, pattern = "[[:punct:]]", replacement = " ")
  tmp <- tolower(tmp)
  email <- tmp
  spam <- if(str_detect(email_list[[i]], "spam")) {"spam"} else {"ham"}
  tmp_corpus <- VCorpus(VectorSource(email))
  corpus <- c(corpus, tmp_corpus)
  spam_labels <- c(spam_labels, spam) #because tm map will strip data, I'm instead making a list of labels
}
```

```{r}
#Clean data
corpus <- tm_map(corpus, function(x) iconv(x, to='UTF-8-MAC', sub='byte')) #removes weird characters
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_Map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, words = stopwords("en"))
corpus <- tm_map(corpus, stemDocument, language = "english")
```

What's the mix of ham vs. spam in our corpus?
```{r}
library(kableExtra)
library(scales)
library(dplyr)
percSpam <- as.data.frame(prop.table(table(spam_labels)))
percSpam %>% 
  rename("Type" = spam_labels) %>% 
  transform(Freq = percent(percSpam$Freq)) %>%  
  kable() %>% kable_styling(bootstrap_options = "basic")
```

Create Term-Document Matrix
```{r}
tdm <- TermDocumentMatrix(corpus)
tdm <- removeSparseTerms(tdm, 1-(10/length(corpus)))
tdm
```

Create Document-Term Matrix
```{r}
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, 1-(10/length(corpus)))

```

#Using RTextTools to create, train, and test a model
##Step 1: Create a "container"
```{r}
library(RTextTools)
N <- 800
end <- length(spam_labels) 
container <-create_container(dtm, 
                             labels = spam_labels, 
                             trainSize = 1:N, 
                             testSize = (N+1):end, 
                             virgin = FALSE)
```

##Step 2: Train each model type
```{r}
maxEntropy_model <- train_model(container, "MAXENT")
svm_model <- train_model(container, "SVM")
randomForest_model <- train_model(container, "TREE")
```

##Step 3: Collect output from each model
```{r}
maxEntropy_out <- classify_model(container, maxEntropy_model)
svm_out <- classify_model(container, svm_model)
randomForest_out <- classify_model(container, randomForest_model)
```

##Step 4: Evaluate results
```{r}
results <- data.frame(correct_label = spam_labels[(N+1):end],
                         maxEntropy = as.character(maxEntropy_out[,1]),
                         svm = as.character(svm_out[,1]),
                         randomForest = as.character(randomForest_out[,1]))

SVM <- as.data.frame(prop.table(table(results$correct_label == results$svm)))
SVM <- sapply(SVM[2], percent)

MaxEntropy <- as.data.frame(prop.table(table(results$correct_label == results$maxEntropy)))
MaxEntropy <- sapply(c(0, MaxEntropy[2]), percent) 

RandomForest <- as.data.frame(prop.table(table(results$correct_label == results$randomForest)))
RandomForest <- sapply(RandomForest[2], percent)


Summary <- data.frame(SVM, MaxEntropy, RandomForest, row.names = c("Error", "Correctly Classified")) 
names(Summary) <- c("SVM", "Max Entropy", "Random Forest")
Summary %>% kable(caption = "Accuracy of Three Model Types in Evaluating Ham vs. Spam") %>% kable_styling(bootstrap_options = "basic")

```

#References
Chapter 10, <i>Automated Data in R</i>

Data source for spam and ham: https://spamassassin.apache.org/old/publiccorpus/

##Useful links
How to untar files in R:
https://stackoverflow.com/questions/7151145/unzip-a-tar-gz-file-in-r

How to load data for text mining:
https://stackoverflow.com/questions/7927367/r-text-file-and-text-mining-how-to-load-data

Dealing with emojis: 
https://stackoverflow.com/questions/9637278/r-tm-package-invalid-input-in-utf8towcs

Troubleshooting:
https://stackoverflow.com/questions/47135684/termdocumentmatrix-not-working-on-corpus
